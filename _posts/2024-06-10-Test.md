---
layout: page
title: Solving "Risk" with Multi-Agent Reinforcement Learning
subtitle: Novel project pushing the bounadries of what is possible in AI today
gh-repo: lpratt30/lpratt30.github.io
---

Our code repository is publicly viewable here: https://github.com/lpratt30/Risk 

Risk, a board game of strategy and negotiations, can be
learned quickly by humans but remains a formidable chal-
lenge for AI. What makes Risk simple from a human per-
spective yet insurmountable for AI? Reinforcement Learn-
ing (RL) models have bested champions in games like Go,
where objective rationality rule move selection, however the
behaviour of the players do not play a role.

Meanwhile, Risk having 6 players adds an element of
mutually assured destruction that exists between any two
players. A player of low skill can ruin the prospects of
a player of the highest skill. This results in emotionally
engaging responses to board states with players of non-
identical skill levels and motivations. There are no “golden
moves” without respect to a given set of players. In such
circumstances the environment and the rules to play by are
defined by the players in it.

This is not unlike daily life where the values people have
create the world we live in and the behaviors we abide by.
This is understood in multi-agent RL as the non-stationarity
problem, but here we highlight the implication that differ-
ent Nash equilibrium of optimal play will exist based on the
motivations of the Agents in the environment. In Risk, play-
ers have different valuations of winning, cooperation, time,
aggression, etc. This has not been considered by prior Risk
research, where it is assumed that all players are perfectly
rational and value the same outcomes to the same extent.
Risk represents a broader class of challenges, which has
been relatively unexplored. Reinforcement learning ap-
proaches to games such as Atari, Chess, Go, and Stratego
have all been for the zero-sum class of problem. Risk has
been incorrectly attributed to be similar but is in fact truly a
novel class of problem.

Our work redefines the problem space by treating Risk
as an extension of research in sequential social dilem-
mas rather than zero-sum games. We argue that the fail-
ure to reach human-level performance in previous research
stems from this misclassification and introduce mixed-
cooperation as a key element in solving Risk. It follows that
there are more parallels with Risk and real-world scenar-
ios involving human decision making than zero-sum games.
One such aspect of this is, unlike 1 v 1 strategy games, Risk
gives players an opportunity to build relationships, show
loyalty, deceive, betray, and take vengeance.
